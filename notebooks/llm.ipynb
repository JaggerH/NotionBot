{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.document_loader import auto_read_pdf\n",
    "path = r\"./data\"\n",
    "documents = auto_read_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([ {'page_content': doc.page_content, 'source': doc.metadata[\"source\"]} for doc in documents])\n",
    "df.to_csv(\"stkj_ocr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarkDown读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "loader = UnstructuredMarkdownLoader('test.md', mode=\"elements\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.md', 'r', encoding='utf-8') as file:\n",
    "    documents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(documents)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=local_model, show_progress=True)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.similarity_search(\"公司主要产品\")\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)  # select top result\n",
    "retriever.batch([\"公司主要产品\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "根据Context回答问题,如果没有相关信息,则回答没有.用中文回答.\n",
    "{question}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "response = rag_chain.invoke(\"公司主营产品是什么\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的将文本进行总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def print_wrapped_text(text, width=80):\n",
    "    for line in text.split('\\n'):\n",
    "        print('\\n'.join([line[i:i+width] for i in range(0, len(line), width)]))\n",
    "\n",
    "summarize_chain = load_summarize_chain(llm)\n",
    "\n",
    "# 定义翻译提示模板\n",
    "translation_prompt = PromptTemplate(input_variables=[\"text\"], template=\"将以下文本翻译成中文：\\n\\n{text}\")\n",
    "# 创建翻译链\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "# 将总结链和翻译链结合\n",
    "def summarize_and_translate(docs):\n",
    "    summary = summarize_chain.run(docs)\n",
    "    translation = translation_chain.run({\"text\": summary})\n",
    "    return translation\n",
    "\n",
    "# 处理文档并获取总结和翻译\n",
    "docs = documents[2:3]\n",
    "# print(docs[0].page_content)\n",
    "translated_summary = summarize_and_translate(docs)\n",
    "print_wrapped_text(translated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Method 1: Stuffing](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb)\n",
    "\n",
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use StuffDocumentsChain as part of the load_summarize_chain method. What you need to do is setting stuff as chain_type of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from utils.chain.load_translate_chain import load_translate_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "pdf_loader = PyPDFLoader('./data/688169_石头科技_北京石头世纪科技股份有限公司第二届董事会第二十五次会议决议公告_1220086493.pdf')\n",
    "docs = pdf_loader.load_and_split()\n",
    "\n",
    "s_chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "t_chain = load_translate_chain(llm)\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "combined_chain = SimpleSequentialChain(chains=[s_chain, t_chain])\n",
    "result = combined_chain.invoke({\"input\": docs})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义的公告总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chain.custom_load_summarize_chain import custom_load_summarize_chain\n",
    "\n",
    "chain = custom_load_summarize_chain(llm)\n",
    "chain.run({\"input_documents\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Mapping, Optional, Protocol\n",
    "\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "prompt_template = \"\"\"用中文简要概括以下内容:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "简要总结:\n",
    "<公司简称><股票代码><公告标题>\n",
    "<公告主要内容>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "def custom_load_summarize_chain(\n",
    "    llm: BaseLanguageModel,\n",
    "    prompt: BasePromptTemplate = PROMPT,\n",
    "    document_variable_name: str = \"text\",\n",
    "    verbose: Optional[bool] = None,\n",
    "    **kwargs: Any,\n",
    ") -> StuffDocumentsChain:\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)  # type: ignore[arg-type]\n",
    "    # TODO: document prompt\n",
    "    return StuffDocumentsChain(\n",
    "        llm_chain=llm_chain,\n",
    "        document_variable_name=document_variable_name,\n",
    "        verbose=verbose,  # type: ignore[arg-type]\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "pdf_loader = PyPDFLoader('./data/688169_石头科技_北京石头世纪科技股份有限公司第二届董事会第二十五次会议决议公告_1220086493.pdf')\n",
    "docs = pdf_loader.load_and_split()\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "# chain = custom_load_summarize_chain(llm)\n",
    "# chain.run({\"input_documents\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "file_path = './data/688169_石头科技_北京石头世纪科技股份有限公司第二届董事会第二十五次会议决议公告_1220086493.pdf'\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "result = chain.run({\"input_documents\": docs})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.encode(docs[0].page_content)\n",
    "token_length = len(tokens)\n",
    "print(f\"Token数量: {token_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 定义文本\n",
    "text = \"年报、半年报、一季报、三季报、业绩预告、权益分派、董事会、监事会、股东大会、日常经营、公司治理、中介报告、首发、增发、股权激励、配股、解禁、公司债、可转债、其他融资、股权变动、补充更正、澄清致歉、风险提示、特别处理和退市、退市整理期\"\n",
    "# 编码文本\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "# 计算token数量\n",
    "token_length = len(tokens)\n",
    "print(f\"Token数量: {token_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: MapReduce\n",
    "The MapReduce method implements a multi-stage summarization. It is a technique for summarizing large pieces of text by first summarizing smaller chunks of text and then combining those summaries into a single summary.\n",
    "\n",
    "In LangChain, you can use MapReduceDocumentsChain as part of the load_summarize_chain method. What you need to do is setting map_reduce as chain_type of your chain.\n",
    "\n",
    "### Prompt design with MapReduce chain\n",
    "In our example, you have a 32-page document that you need to summarize.\n",
    "\n",
    "With LangChain, the map_reduce chain breaks the document down into 1024 token chunks max. Then it runs the initial prompt you define on each chunk to generate a summary of that chunk. In the example below, you use the following first stage or map prompt.\n",
    "```\n",
    "Write\n",
    "'''{text}'''. BULLET POINT SUMMARY:```\n",
    "\n",
    "Once summaries for all of the chunks are generated, it runs a different prompt to combine those summaries into a single summary. In the example below, you use the following second stage or combine prompt.\n",
    "\n",
    "```Write a summary of the entire document that includes the main points from all of the individual summaries.```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('./data/688169_石头科技_中信证券股份有限公司关于北京石头世纪科技股份有限公司2023年度募集资金存放与实际使用情况的专项核查意见_1219442733.pdf')\n",
    "docs = pdf_loader.load_and_split()\n",
    "# print(pages[3].page_content)\n",
    "for page in docs:\n",
    "    print(type(page), page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama2-chinese:13b\", temperature=0)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "from transformers import GPT2TokenizerFast\n",
    "# 加载预训练的 GPT-2 分词器\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"../gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Option 3. Refine](https://python.langchain.com/v0.2/docs/tutorials/summarization/#refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "result = chain.invoke(split_docs)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 获取 HF_HOME 环境变量的值\n",
    "os.getenv('HF_HOME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "local_model = \"llama2-chinese:13b\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=local_model, show_progress=True)\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = db.similarity_search(\"石头科技2023年年度报告，管理层分析\", k=4)\n",
    "for doc in query_result:\n",
    "    print(doc)\n",
    "    # print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Set up the local model:\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=local_model, num_predict=400,\n",
    "                 stop=[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set up the RAG chain:\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Answer the user's question using provided context. Stick to the facts, do not draw your own conclusions.\n",
    "Question: {question}\n",
    "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"石头科技2023年年报，管理层讨论一节都说了些什么\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Annoy\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import time\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def create_index(documents):    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name='moka-ai/m3e-large')\n",
    "    vector_store_path = r\"./storage4\"\n",
    "\n",
    "    docsearch = Annoy.from_documents(documents=split_docs,\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=vector_store_path)\n",
    "    docsearch.save_local(vector_store_path)\n",
    "\n",
    "def search(txt):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='moka-ai/m3e-large')\n",
    "    vector_store_path = r\"./storage4\"\n",
    "    docsearch = Annoy.load_local(vector_store_path,embeddings=embeddings)\n",
    "\n",
    "    start = time.time()\n",
    "    prompt_template = \"\"\"请注意：请谨慎评估query与提示的Context信息的相关性，只根据本段输入文字信息的内容进行回答，如果query与提供的材料无关，请回答\"对不起，我不知道\"，另外也不要回答无关答案：\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    # qa = VectorDBQA.from_chain_type(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\"), chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "    # result = qa({\"query\": txt})\n",
    "    \n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"), chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 8}),\n",
    "                                 chain_type_kwargs={\"prompt\": PROMPT})\n",
    "    \n",
    "    result = qa.run(txt)\n",
    "    print(result)\n",
    "    print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文档\n",
    "https://github.com/ollama/ollama/blob/main/examples/langchain-python-rag-document/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "# load the pdf and split it into chunks\n",
    "loader = OnlinePDFLoader(\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001813756/975b3e9b-268e-4798-a9e4-2a9a7c92dc10.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "with SuppressStdout():\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nQuery: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    llm = Ollama(model=\"llama3:8b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NotionBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
